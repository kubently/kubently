Persona: You are an expert Python developer with deep experience in building and debugging applications powered by Large Language Models (LLMs) and frameworks like LangChain/LangGraph. You are meticulous about observability and understand that high-quality, structured logging is essential for prompt engineering and improving agent performance.

Primary Goal: Your goal is to significantly enhance the logging within the kubently A2A (Agent-to-Agent) agent. The objective is to provide deep, structured visibility into the agent's entire reasoning and execution cycle, which is crucial for debugging its behavior and refining its system prompt.

Context: The current logging in the agent is too high-level. It does not capture the specific inputs and outputs of the LLM or its tools, making it difficult to diagnose why the agent makes certain decisions. You will modify the core agent logic to introduce detailed, structured logging that illuminates its "thought process."

Target File: All modifications will be made within the following file: kubently/modules/a2a/protocol_bindings/a2a_server/agent.py.

Key Requirements & Implementation Steps:

Implement Conditional, Structured Logging:

All new, detailed logging must be conditional and only activate when the environment variable A2A_SERVER_DEBUG is set to "true".

Create a helper function, for example structured_log(log_data: dict), that takes a dictionary, adds a unique trace ID for the request, and logs it as a formatted JSON string. This ensures logs are easily parseable.

Log the Full LLM Prompt:

Location: Inside the run method, immediately before the self.agent.astream(...) call.

Action: Log the complete payload being sent to the LLM. This is the most critical log for prompt engineering.

Log Content: The structured log should include keys like "event": "llm_prompt", "thread_id", "system_prompt", and the full, un-truncated "messages" history.

Log Raw Tool Calls from the LLM:

Location: Inside the async for loop within the run method.

Action: When an AIMessage containing tool_calls is received from the agent stream, log the raw tool_calls attribute.

Log Content: The structured log should include "event": "llm_raw_tool_calls", the "thread_id", and the complete tool_calls data structure. This shows exactly how the LLM decided to invoke a function.

Log Full Tool Inputs and Outputs:

Location: Inside the execute_kubectl tool function (and any other tools).

Action:

At the beginning of the function, log the parameters it was called with.

Before returning, log the entire, un-truncated output received from the kubectl command or any other external call.

Log Content: Use structured logs with events like "tool_input" and "tool_output", including the tool name, parameters, and the full output string.

Log the Final Agent Response and Errors:

Location: At the end of the run method.

Action: Log the final, complete response generated by the agent that will be sent to the user. Also, ensure any exceptions caught during the agent's execution are logged in a structured format.

Log Content: Use events like "llm_final_response" and "agent_error", including the "thread_id", and the full response content or error message.

Final Deliverable:
Provide the complete, modified Python code for the kubently/modules/a2a/protocol_bindings/a2a_server/agent.py file. The new code must incorporate all the logging enhancements as described above, ensuring that running the service with A2A_SERVER_DEBUG=true produces a rich, structured, JSON-based log stream that details every step of the agent's process.